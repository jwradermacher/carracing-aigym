{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ue77sh/anaconda3/envs/aigym/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# Make car_racing env\n",
    "env = gym.make('CarRacing-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Box(3,)\n",
      "State Space Box(96, 96, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = {\n",
    "    0: [-.9,  0, 0],\n",
    "    1: [-.45, 0, 0],\n",
    "    2: [0,    0, 0],\n",
    "    3: [.45,  0, 0],\n",
    "    4: [.9,   0, 0],\n",
    "    5: [-.9, .6,.1],\n",
    "    6: [-.45,.6,.1],\n",
    "    7: [0,   .6,.1],\n",
    "    8: [.45, .6,.1],\n",
    "    9: [.9,  .6,.1],\n",
    "    10:[-.9, .9,.1],\n",
    "    11:[-.45,.9,.1],\n",
    "    12:[0,   .9,.1],\n",
    "    13:[.45, .9,.1],\n",
    "    14:[.9,  .9,.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_num =  512 # 2ˆˆ9\n",
    "q_table = np.zeros([state_num, len(actions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 15)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1146..1447 -> 301-tiles track\n",
      "Track generation: 1092..1369 -> 277-tiles track\n",
      "Track generation: 1212..1519 -> 307-tiles track\n",
      "Track generation: 1180..1479 -> 299-tiles track\n",
      "Track generation: 1064..1334 -> 270-tiles track\n",
      "Track generation: 1209..1515 -> 306-tiles track\n",
      "Track generation: 1201..1509 -> 308-tiles track\n",
      "retry to generate track (normal if there are not many of this messages)\n",
      "Track generation: 1231..1543 -> 312-tiles track\n",
      "Track generation: 1089..1365 -> 276-tiles track\n",
      "Track generation: 1072..1344 -> 272-tiles track\n",
      "Track generation: 1151..1443 -> 292-tiles track\n",
      "Track generation: 1092..1369 -> 277-tiles track\n",
      "Track generation: 1150..1441 -> 291-tiles track\n",
      "Track generation: 1213..1520 -> 307-tiles track\n",
      "Track generation: 1089..1366 -> 277-tiles track\n",
      "Track generation: 1084..1358 -> 274-tiles track\n",
      "Track generation: 1349..1691 -> 342-tiles track\n",
      "Track generation: 1100..1379 -> 279-tiles track\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m_ctypes/callbacks.c\u001b[0m in \u001b[0;36m'calling callback function'\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aigym/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36mobjc_method\u001b[0;34m(objc_self, objc_cmd, *args)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mpy_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjCInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjc_self\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0mpy_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjc_cmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_method_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObjCClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aigym/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36mconvert_method_arguments\u001b[0;34m(encoding, args)\u001b[0m\n\u001b[1;32m    998\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_encodings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'@'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m             \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjCInstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb'#'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0mnew_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjCClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aigym/lib/python3.8/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, object_ptr)\u001b[0m\n\u001b[1;32m    919\u001b[0m         for the given object_ptr which should be an Objective-C id.\"\"\"\n\u001b[1;32m    920\u001b[0m         \u001b[0;31m# Make sure that object_ptr is wrapped in a c_void_p.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mobject_ptr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_void_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "\n",
    "# Outer loop is for multiple simulations (increase for learning over a long time)\n",
    "for i in range(20):\n",
    "    state = env.reset() # Reset to start a new simulation\n",
    "\n",
    "    # Inner loop for simulating the environment:\n",
    "    # 1. Observe 2. Take action 3. Read reward 4. Learn 1. Observe ...\n",
    "    for i in range(1000):\n",
    "        env.render() # For seeing the simulation step\n",
    "        discrete_state = to_num(binarize(state))\n",
    "\n",
    "        #epochs, penalties, reward, = 0, 0, 0\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_idx = random.choice(list(actions.keys()))\n",
    "            action = actions.get(action_idx)\n",
    "        else:\n",
    "            action_idx = np.argmax(q_table[discrete_state])\n",
    "            action = actions.get(action_idx)\n",
    "        \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        discrete_next_state = to_num(binarize(next_state))\n",
    "        \n",
    "        old_value = q_table[discrete_state][action_idx]\n",
    "        next_max = np.max(q_table[discrete_next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[discrete_state][action_idx] = new_value\n",
    "        \n",
    "        state = next_state\n",
    "                    \n",
    "        # Pring the observations, rewards, whether the round has ended\n",
    "        \"\"\"\n",
    "        print(\"Observation: \", discrete_next_state,\n",
    "              \"Reward: \", reward,\n",
    "              \"Done?: \", done,\n",
    "              \"Action taken: \", action)\n",
    "        \"\"\"\n",
    "        \n",
    "        if i < 1000: # Calculate reward for only first 2000 episodes\n",
    "            total_reward += reward\n",
    "        \n",
    "        if done: # Car steered off stop the simulation and restart it\n",
    "            break\n",
    "            \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1094..1379 -> 285-tiles track\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'next_state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-581ee1f6b2c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mold_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mnext_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mold_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'next_state' is not defined"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "\n",
    "# Outer loop is for multiple simulations (increase for learning over a long time)\n",
    "for i in range(10):\n",
    "    env.reset() # Reset to start a new simulation\n",
    "    \n",
    "    # Inner loop for simulating the environment:\n",
    "    # 1. Observe 2. Take action 3. Read reward 4. Learn 1. Observe ...\n",
    "    for i in range(100):\n",
    "        state = i\n",
    "        env.render() # For seeing the simulation step\n",
    "        \n",
    "        #epochs, penalties, reward, = 0, 0, 0\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action_idx = random.choice(list(actions.keys()))\n",
    "            action = actions.get(action_idx)\n",
    "        else:\n",
    "            action_idx = np.argmax(q_table[state])\n",
    "            action = actions.get(action_idx)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        old_value = q_table[state][action_idx]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state][action_idx] = new_value\n",
    "        \n",
    "        state = next_state\n",
    "                    \n",
    "        # Pring the observations, rewards, whether the round has ended\n",
    "        print(\"Observation: \", observation.shape,\n",
    "              \"Reward: \", reward,\n",
    "              \"Done?: \", done)\n",
    "        \n",
    "        if i < 1000: # Calculate reward for only first 2000 episodes\n",
    "            total_reward += reward\n",
    "        \n",
    "        if done: # Car steered off stop the simulation and restart it\n",
    "            break\n",
    "            \n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete(elem):\n",
    "    res = [0]\n",
    "    am = np.argmax(elem)\n",
    "    if am < 1:\n",
    "        res[0] = 1\n",
    "    return res\n",
    "\n",
    "def binarize(obs):\n",
    "    # Binarize\n",
    "    res = ds = np.array(\n",
    "        list(\n",
    "            map(discrete, obs.reshape(96 * 96, 3))\n",
    "        )\n",
    "    ).reshape(96, 96, 1)\n",
    "    \n",
    "    res = res[15:-15, 15:-15, :]\n",
    "    \n",
    "    # Downsample\n",
    "    acc = res[0::22, 0::22, 0]\n",
    "    for i in range(1,22):\n",
    "        for j in range(1,22):\n",
    "            acc += res[i::22, j::22, 0]\n",
    "    \n",
    "    acc = (acc / (22 * 22)) > 0.3\n",
    "\n",
    "    #acc = acc[:-1,:]\n",
    "    \n",
    "    return np.array(acc, dtype=int)\n",
    "\n",
    "def to_num(obs):\n",
    "    rav = obs.ravel()\n",
    "    res = 0\n",
    "    for i in range(len(rav)):\n",
    "        res = res + rav[i] * 2**i\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
